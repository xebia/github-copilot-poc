# 12. LLM basics, models, and Premium Requests (practical)

## Goal
Understand what “model choice” affects, and how to work efficiently with Premium Requests.

## Time
10–15 minutes

## LLM basics (only what you need)
- Copilot uses a **language model** that predicts likely next text based on your prompt + context.
- Different models can behave differently: speed, code quality, reasoning depth.

## Models (hands-on)
1. In Copilot Chat, locate the **model picker** (if available).
2. Ask the same question with two different models:
   - “Add a new endpoint to PlanesController that searches by year. Keep it simple.”
3. Compare:
   - verbosity
   - correctness
   - style consistency

## Premium Requests (workshop-ready guidance)
- Use Premium Requests for tasks that benefit from deeper reasoning (debugging, multi-file understanding).
- Use standard requests for quick completions and small edits.
- **Check your quota:** Look for usage info in VS Code (Copilot status) or your GitHub account settings under Copilot.
- **When exhausted:** Copilot falls back to a standard model; completions still work.
 - **Plan-dependent:** Availability and quota for Premium Requests depend on your GitHub Copilot plan and organization settings.

## Prompt to practice (optimize cost/value)
- “Before writing code, summarize the plan in 3 bullets. Then implement only the controller method.”

## Success criteria
- You can explain why you might switch models.
- You can intentionally choose “small prompt” vs “deep prompt”.
